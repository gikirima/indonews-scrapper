{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f961a6c1-4d6a-40ce-bd84-96a6e3d0e3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Memuat dataset dari 'dataset_baru.csv'...\n",
      "Langkah 1: Menghitung token menggunakan tokenizer 'cl100k_base' (untuk model GPT-3.5/4)...\n",
      "                                               judul  jumlah_token_chatgpt\n",
      "0  Kisah 'Kuli Bangunan' Bawa UMKM Bengkulu Naik ...                  2290\n",
      "1  Gunung Raung Erupsi, Kolom Abu Capai 2 Ribu Meter                   489\n",
      "2  Jorge Martin Dirumorkan Gabung Honda di MotoGP...                   399\n",
      "3  PKS Minta Pemerintah Siapkan Skenario Darurat ...                   694\n",
      "4  Malam Ini Unduh Mantu Besar dan Megah Al Ghaza...                   759\n",
      "  - Menambahkan 430 token prompt ke setiap baris...\n",
      "Penghitungan token selesai.\n",
      "\n",
      "Langkah 2: Membagi dataset menjadi batch dengan batas token 2.0M...\n",
      "Dataset berhasil dibagi menjadi 1 batch.\n",
      "\n",
      "Langkah 3: Menyimpan setiap batch ke file CSV terpisah...\n",
      "  -> Batch 1 disimpan ke 'output_batches_chatgpt/dataset_part_1.csv' (1460 baris, Total Token: 1923153)\n",
      "\n",
      "Proses selesai!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tiktoken\n",
    "import os\n",
    "\n",
    "# Tentukan model yang akan Anda gunakan. Ini penting karena beda model, beda cara tokenisasi.\n",
    "# \"cl100k_base\" adalah encoding yang digunakan oleh model gpt-4, gpt-3.5-turbo, dan text-embedding-ada-002\n",
    "try:\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saat memuat encoding tiktoken: {e}\")\n",
    "    exit()\n",
    "\n",
    "def hitung_token_chatgpt(teks):\n",
    "    \"\"\"\n",
    "    Fungsi untuk menghitung jumlah token dari sebuah teks menggunakan tokenizer resmi OpenAI.\n",
    "    \"\"\"\n",
    "    if not isinstance(teks, str):\n",
    "        return 0\n",
    "    # Encode teks menjadi nomor token, lalu hitung jumlahnya\n",
    "    return len(encoding.encode(teks))\n",
    "\n",
    "\n",
    "# Masukkan path menuju file input\n",
    "NAMA_FILE_INPUT = 'dataset_baru.csv'\n",
    "TOKEN_PROMPT_PERINTAH = 430  # Definisikan token prompt sebagai variabel\n",
    "\n",
    "# --- Mulai Proses Utama ---\n",
    "try:\n",
    "    print(f\"\\nMemuat dataset dari '{NAMA_FILE_INPUT}'...\")\n",
    "    df = pd.read_csv(NAMA_FILE_INPUT)\n",
    "\n",
    "    # 1. Hitung token menggunakan TIKTOKEN dan tambahkan ke kolom baru\n",
    "    print(\"Langkah 1: Menghitung token menggunakan tokenizer 'cl100k_base' (untuk model GPT-3.5/4)...\")\n",
    "    df['jumlah_token_chatgpt'] = df['isi_berita'].apply(hitung_token_chatgpt)\n",
    "    print(df[['judul', 'jumlah_token_chatgpt']].head())\n",
    "\n",
    "    print(f\"  - Menambahkan {TOKEN_PROMPT_PERINTAH} token prompt ke setiap baris...\")\n",
    "    # Tambahkan jumlah token prompt ke setiap baris secara efisien\n",
    "    df['jumlah_token_chatgpt'] += TOKEN_PROMPT_PERINTAH\n",
    "    print(\"Penghitungan token selesai.\")\n",
    "\n",
    "    # 2. Bagi dataset menjadi beberapa batch berdasarkan jumlah token\n",
    "    print(\"\\nLangkah 2: Membagi dataset menjadi batch dengan batas token 2.0M...\")\n",
    "    TOKEN_LIMIT = 2_000_000\n",
    "    batches = []\n",
    "    current_batch_start_index = 0\n",
    "    current_token_count = 0\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        row_token_count = row['jumlah_token_chatgpt']\n",
    "        \n",
    "        if current_token_count > 0 and (current_token_count + row_token_count) > TOKEN_LIMIT:\n",
    "            batch_df = df.iloc[current_batch_start_index:index]\n",
    "            batches.append(batch_df)\n",
    "            current_batch_start_index = index\n",
    "            current_token_count = 0\n",
    "\n",
    "        current_token_count += row_token_count\n",
    "\n",
    "    # Simpan batch terakhir\n",
    "    last_batch_df = df.iloc[current_batch_start_index:]\n",
    "    if not last_batch_df.empty:\n",
    "        batches.append(last_batch_df)\n",
    "\n",
    "    print(f\"Dataset berhasil dibagi menjadi {len(batches)} batch.\")\n",
    "\n",
    "    # 3. Outputkan setiap batch ke file CSV baru\n",
    "    print(\"\\nLangkah 3: Menyimpan setiap batch ke file CSV terpisah...\")\n",
    "    output_dir = \"output_batches_chatgpt\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for i, batch_df in enumerate(batches):\n",
    "        nama_file_output = os.path.join(output_dir, f'dataset_part_{i+1}.csv')\n",
    "        batch_df.to_csv(nama_file_output, index=False)\n",
    "        total_tokens_in_batch = batch_df['jumlah_token_chatgpt'].sum()\n",
    "        print(f\"  -> Batch {i+1} disimpan ke '{nama_file_output}' ({len(batch_df)} baris, Total Token: {total_tokens_in_batch:n})\")\n",
    "\n",
    "    print(\"\\nProses selesai!\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File '{NAMA_FILE_INPUT}' tidak ditemukan.\")\n",
    "except KeyError:\n",
    "    print(f\"Error: Dataset Anda tidak memiliki kolom 'isi_berita'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8611c91f-8412-480e-b940-941a3edd3ef2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
