{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f73590a5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f73590a5",
        "outputId": "f7f7c0ea-ffed-4104-a80d-184b6b434c11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting sgmllib3k (from feedparser)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=45d31e891dad1c5cf7290833f7bf6d0fc910719e2235846e7dc2eb29ed392ad9\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser\n",
            "Successfully installed feedparser-6.0.11 sgmllib3k-1.0.0\n"
          ]
        }
      ],
      "source": [
        "%pip install feedparser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b494b56a",
      "metadata": {
        "id": "b494b56a"
      },
      "outputs": [],
      "source": [
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "from urllib.parse import urlparse\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import feedparser\n",
        "import pandas as pd\n",
        "import time\n",
        "import re\n",
        "import concurrent.futures\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "069d0279",
      "metadata": {
        "id": "069d0279"
      },
      "outputs": [],
      "source": [
        "# %%\n",
        "%pip install feedparser\n",
        "# %%\n",
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "from urllib.parse import urlparse\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import feedparser\n",
        "import pandas as pd\n",
        "import time\n",
        "import re\n",
        "import concurrent.futures\n",
        "\n",
        "# %%\n",
        "def parse_tribun(soup, url, source):\n",
        "    \"\"\"\n",
        "    Mengurai objek BeautifulSoup untuk website tribunnews.com.\n",
        "\n",
        "    Args:\n",
        "        soup (BeautifulSoup object): Objek BeautifulSoup dari halaman yang di-scrape.\n",
        "        url (str): URL artikel.\n",
        "        source (str): Nama sumber website.\n",
        "\n",
        "    Returns:\n",
        "        dict or None: Dictionary data artikel jika berhasil, None jika gagal.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        judul_tag = soup.select_one(\"h1\")\n",
        "        if not judul_tag:\n",
        "            print(\"  ⚠️ Tidak menemukan tag <h1> untuk tribunnews.com\")\n",
        "            return None\n",
        "\n",
        "        isi_paragraf = soup.select(\".side-article p\")\n",
        "        if not isi_paragraf:\n",
        "            print(\"  ⚠️ Tidak menemukan isi artikel untuk tribunnews.com\")\n",
        "            return None\n",
        "\n",
        "        judul = judul_tag.text.strip()\n",
        "        isi = \"\\n\".join([p.text.strip() for p in isi_paragraf if p.text.strip()])\n",
        "\n",
        "        # Tanggal dan waktu\n",
        "        tanggal_tag = soup.select_one(\"time\") or soup.select_one(\".date\")\n",
        "        tanggal_full = tanggal_tag.text.strip() if tanggal_tag else \"Tidak ditemukan\"\n",
        "        match = re.search(r\"(\\d{1,2}/\\d{1,2}/\\d{4})[ ,]*(\\d{1,2}:\\d{2})?\", tanggal_full)\n",
        "        date = match.group(1) if match else tanggal_full\n",
        "        time_ = match.group(2) if match and match.group(2) else \"\"\n",
        "\n",
        "        # Breadcrumb: kategori dan sub-kategori\n",
        "        breadcrumb = soup.select(\".breadcrumb li a\")\n",
        "        category = breadcrumb[1].text.strip() if len(breadcrumb) > 1 else \"\"\n",
        "        sub_category = breadcrumb[2].text.strip() if len(breadcrumb) > 2 else \"\"\n",
        "\n",
        "        return {\n",
        "            \"title\": judul,\n",
        "            \"source\": source,\n",
        "            \"date\": date,\n",
        "            \"time\": time_,\n",
        "            \"category\": category,\n",
        "            \"sub-category\": sub_category,\n",
        "            \"content\": isi,\n",
        "            \"url\": url,\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ❌ Gagal mengurai tribunnews.com {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# %%\n",
        "def parse_sindonews(soup, url, source):\n",
        "    \"\"\"\n",
        "    Mengurai objek BeautifulSoup untuk website sindonews.com.\n",
        "\n",
        "    Args:\n",
        "        soup (BeautifulSoup object): Objek BeautifulSoup dari halaman yang di-scrape.\n",
        "        url (str): URL artikel.\n",
        "        source (str): Nama sumber website.\n",
        "\n",
        "    Returns:\n",
        "        dict or None: Dictionary data artikel jika berhasil, None jika gagal.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        judul_tag = soup.select_one(\".detail-title\")\n",
        "        if not judul_tag:\n",
        "             print(\"  ⚠️ Tidak menemukan .detail-title untuk sindonews.com\")\n",
        "             return None\n",
        "\n",
        "        tanggal_tag = soup.select_one(\".detail-date-artikel\")\n",
        "        if not tanggal_tag:\n",
        "            print(\"  ⚠️ Tidak menemukan .detail-date-artikel untuk sindonews.com\")\n",
        "            # Masih bisa lanjut tanpa tanggal/waktu, beri nilai default\n",
        "            tanggal_full = \"Tidak ditemukan\"\n",
        "        else:\n",
        "            tanggal_full = tanggal_tag.text.strip()\n",
        "\n",
        "\n",
        "        isi_div = soup.select_one(\"#detail-desc\")\n",
        "        if not isi_div:\n",
        "            print(\"  ⚠️ Tidak menemukan #detail-desc untuk sindonews.com\")\n",
        "            return None # Konten adalah elemen penting\n",
        "\n",
        "        judul = judul_tag.text.strip()\n",
        "\n",
        "        # Mengambil teks dari semua paragraf di dalam div #detail-desc\n",
        "        isi = \"\\n\".join([p.text.strip() for p in isi_div.find_all(\"p\") if p.text.strip()])\n",
        "\n",
        "        # Parsing tanggal dan waktu\n",
        "        match = re.search(r\"(\\d{1,2}/\\d{1,2}/\\d{4})[ ,]*(\\d{1,2}:\\d{2})?\", tanggal_full)\n",
        "        date = match.group(1) if match else tanggal_full\n",
        "        time_ = match.group(2) if match and match.group(2) else \"\"\n",
        "\n",
        "        # Kategori dan Sub-kategori (Perlu dicari selector yang sesuai di Sindonews)\n",
        "        # Untuk contoh, kita bisa coba cari di breadcrumb atau meta tag\n",
        "        # Jika sulit ditemukan selector umum, mungkin perlu diatur manual atau dibiarkan kosong.\n",
        "        # Contoh: mencari di breadcrumb\n",
        "        breadcrumb = soup.select(\".breadcrumb li a\") # Coba selector umum, mungkin beda\n",
        "        category = breadcrumb[1].text.strip() if len(breadcrumb) > 1 else \"\"\n",
        "        sub_category = breadcrumb[2].text.strip() if len(breadcrumb) > 2 else \"\"\n",
        "\n",
        "        # Jika breadcrumb di atas tidak bekerja, bisa coba meta tag atau bagian lain di HTML\n",
        "        # meta_category = soup.select_one('meta[property=\"article:section\"]')\n",
        "        # category = meta_category['content'] if meta_category and 'content' in meta_category.attrs else \"\"\n",
        "\n",
        "        return {\n",
        "            \"title\": judul,\n",
        "            \"source\": source,\n",
        "            \"date\": date,\n",
        "            \"time\": time_,\n",
        "            \"category\": category, # Mungkin perlu disesuaikan\n",
        "            \"sub-category\": sub_category, # Mungkin perlu disesuaikan\n",
        "            \"content\": isi,\n",
        "            \"url\": url,\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ❌ Gagal mengurai sindonews.com {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# %%\n",
        "def scrape_berita(url, source):\n",
        "    \"\"\"\n",
        "    Mengambil HTML dan memanggil fungsi parsing yang sesuai berdasarkan sumber.\n",
        "\n",
        "    Args:\n",
        "        url (str): URL artikel.\n",
        "        source (str): Nama sumber website.\n",
        "\n",
        "    Returns:\n",
        "        dict or None: Dictionary data artikel jika berhasil di-scrape dan diurai, None jika gagal.\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\",\n",
        "        \"Referer\": \"https://www.google.com/\",\n",
        "        \"Accept-Language\": \"id,en-US;q=0.9,en;q=0.8\",\n",
        "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
        "    }\n",
        "    try:\n",
        "        resp = requests.get(url, headers=headers, timeout=10)\n",
        "        if resp.status_code != 200:\n",
        "            print(f\"  ⚠️ Status code: {resp.status_code} for {url}\")\n",
        "            return None\n",
        "        soup = BeautifulSoup(resp.content, \"html.parser\")\n",
        "\n",
        "        # Pilih fungsi parsing berdasarkan sumber\n",
        "        if \"tribunnews.com\" in source:\n",
        "            return parse_tribun(soup, url, source)\n",
        "        elif \"sindonews.com\" in source:\n",
        "            return parse_sindonews(soup, url, source)\n",
        "        # Tambahkan elif untuk website lain di sini\n",
        "        # elif \"namadomainlain.com\" in source:\n",
        "        #    return parse_namadomainlain(soup, url, source)\n",
        "        else:\n",
        "            print(f\"  ⚠️ Sumber tidak dikenali untuk parsing: {source}\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ❌ Gagal mengambil atau memproses HTML dari {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# %%\n",
        "def full_rss_scrape(list_of_rss_urls, output_csv=\"berita.csv\", max_articles=1000):\n",
        "    \"\"\"\n",
        "    Mengambil link artikel dari daftar URL RSS, kemudian melakukan scraping,\n",
        "    dan menyimpan hasilnya ke file CSV.\n",
        "\n",
        "    Args:\n",
        "        list_of_rss_urls (list): Daftar URL RSS yang akan diambil link-nya.\n",
        "        output_csv (str): Nama file CSV untuk menyimpan hasil scraping.\n",
        "        max_articles (int): Jumlah maksimum artikel yang akan di-scrape dari link RSS.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: DataFrame yang berisi data artikel yang berhasil di-scrape.\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
        "        \"Referer\": \"https://www.google.com/\",\n",
        "    }\n",
        "\n",
        "    urls = set()\n",
        "    print(\"➡️ Mengumpulkan link dari RSS...\")\n",
        "    for rss_url in list_of_rss_urls:\n",
        "        try:\n",
        "            print(f\"  Mengambil dari: {rss_url}\")\n",
        "            resp = requests.get(rss_url, headers=headers, timeout=10)\n",
        "            feed = feedparser.parse(resp.text)\n",
        "            for entry in feed.entries:\n",
        "                # Hanya tambahkan jika link tidak kosong\n",
        "                if entry.link:\n",
        "                  urls.add(entry.link)\n",
        "            time.sleep(1) # Memberi jeda agar tidak terlalu cepat\n",
        "        except Exception as e:\n",
        "            print(f\"  ❌ Gagal ambil dari {rss_url}: {e}\")\n",
        "\n",
        "    # Ambil sejumlah URL sesuai parameter max_articles\n",
        "    urls_to_scrape = list(urls)[:max_articles]\n",
        "    print(f\"✅ Total link unik yang berhasil dikumpulkan: {len(urls)}\")\n",
        "    print(f\"➡️ Akan men-scrape {len(urls_to_scrape)} artikel...\")\n",
        "\n",
        "    print(\"➡️ Memulai proses scraping artikel...\")\n",
        "    def scrape_wrapper(args):\n",
        "        url, source = args\n",
        "        print(f\"  Scraping: {url} | Source: {source}\")\n",
        "        # Memanggil fungsi scrape_berita yang sudah diperbarui\n",
        "        return scrape_berita(url, source)\n",
        "\n",
        "    berita_list = []\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        # Menggunakan urlparse untuk mendapatkan source (nama domain)\n",
        "        args_list = [(url, urlparse(url).netloc) for url in urls_to_scrape]\n",
        "        results = list(executor.map(scrape_wrapper, args_list))\n",
        "        for data in results:\n",
        "            if data:\n",
        "                berita_list.append(data)\n",
        "\n",
        "    df = pd.DataFrame(berita_list, columns=[\n",
        "        \"title\", \"source\", \"date\", \"time\", \"category\", \"sub-category\", \"content\", \"url\"\n",
        "    ])\n",
        "\n",
        "    # Simpan hasil ke CSV\n",
        "    print(f\"➡️ Menyimpan hasil ke {output_csv}...\")\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    print(f\"✅ CSV disimpan! Jumlah artikel berhasil di-scrape: {len(df)}\")\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "93525a52",
      "metadata": {
        "id": "93525a52"
      },
      "outputs": [],
      "source": [
        "# Contoh penggunaan fungsi full_rss_scrape:\n",
        "\n",
        "# Ambil daftar link RSS dari file CSV (misalnya hanya untuk 'tribun')\n",
        "dataset = pd.read_csv('https://raw.githubusercontent.com/gikirima/indonews-scrapper/refs/heads/main/link_scrapping.csv')\n",
        "tribun_rss_links = dataset[dataset['website'] == 'tribun']['link'].tolist()\n",
        "sindonews_rss_links = dataset[dataset['website'] == 'sindonews']['link'].tolist()\n",
        "liputan6_rss_links = dataset[dataset['website'] == 'liputan6']['link'].tolist()\n",
        "detik_rss_links = dataset[dataset['website'] == 'detik']['link'].tolist()\n",
        "kapanlagi_rss_links = dataset[dataset['website'] == 'kapanlagi']['link'].tolist()\n",
        "fimela_rss_links = dataset[dataset['website'] == 'fimela']['link'].tolist()\n",
        "okezone_rss_links = dataset[dataset['website'] == 'okezone']['link'].tolist()\n",
        "posmetro_rss_links = dataset[dataset['website'] == 'posmetro']['link'].tolist()\n",
        "kompas_rss_links = dataset[dataset['website'] == 'kompas']['link'].tolist()\n",
        "republika_rss_links = dataset[dataset['website'] == 'republika']['link'].tolist()\n",
        "tempo_rss_links = dataset[dataset['website'] == 'tempo']['link'].tolist()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Panggil fungsi full_rss_scrape dengan daftar link RSS\n",
        "# Anda bisa menyesuaikan nama file output CSV dan jumlah maksimum artikel\n",
        "df_tribun = full_rss_scrape(tribun_rss_links, output_csv=\"berita_tribun.csv\", max_articles=500)\n",
        "\n",
        "# Jika Anda ingin melihat DataFrame hasilnya:\n",
        "display(df_tribun.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJUF-DaAqDWH",
        "outputId": "08381702-44a3-4bfb-9f41-d694dba6e2a8"
      },
      "id": "XJUF-DaAqDWH",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['https://rss.tempo.co/',\n",
              " 'https://rss.tempo.co/politik',\n",
              " 'https://rss.tempo.co/hukum',\n",
              " 'https://rss.tempo.co/ekonomi',\n",
              " 'https://rss.tempo.co/lingkungan',\n",
              " 'https://rss.tempo.co/wawancara',\n",
              " 'https://rss.tempo.co/investigasi',\n",
              " 'https://rss.tempo.co/cekfakta',\n",
              " 'https://rss.tempo.co/tokoh',\n",
              " 'https://rss.tempo.co/newsletter',\n",
              " 'https://rss.tempo.co/info-tempo']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_sindonews = full_rss_scrape(sindonews_rss_links, output_csv=\"berita_sindonews\", max_articles=500)\n",
        "df_liputan6 = full_rss_scrape(liputan6_rss_links, output_csv=\"berita_liputan6\", max_articles=500)\n",
        "df_detik = full_rss_scrape(detik_rss_links, output_csv=\"berita_detik\", max_articles=500)\n",
        "df_kapanlagi = full_rss_scrape(kapanlagi_rss_links, output_csv=\"berita_kapanlagi\", max_articles=500)\n",
        "df_fimela = full_rss_scrape(fimela_rss_links, output_csv=\"berita_fimela\", max_articles=500)\n",
        "df_okezone = full_rss_scrape(okezone_rss_links, output_csv=\"berita_okezone\", max_articles=500)\n",
        "df_posmetro = full_rss_scrape(posmetro_rss_links, output_csv=\"berita_posmetro\", max_articles=500)\n",
        "df_kompas = full_rss_scrape(kompas_rss_links, output_csv=\"berita_kompas\", max_articles=500)\n",
        "df_republika = full_rss_scrape(republika_rss_links, output_csv=\"berita_republika\", max_articles=500)\n",
        "df_tempo = full_rss_scrape(tempo_rss_links, output_csv=\"berita_tempo\", max_articles=500)"
      ],
      "metadata": {
        "id": "w4MLaEXGrEF9"
      },
      "id": "w4MLaEXGrEF9",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}