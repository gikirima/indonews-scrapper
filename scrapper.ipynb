{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "f73590a5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f73590a5",
        "outputId": "cb58a575-add9-44ba-c016-f8ac540c7eaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: feedparser in ./.local/lib/python3.10/site-packages (6.0.11)\n",
            "Requirement already satisfied: sgmllib3k in ./.local/lib/python3.10/site-packages (from feedparser) (1.0.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install feedparser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "b494b56a",
      "metadata": {
        "id": "b494b56a"
      },
      "outputs": [],
      "source": [
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "from urllib.parse import urlparse\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import feedparser\n",
        "import pandas as pd\n",
        "import time\n",
        "import re\n",
        "import concurrent.futures\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "069d0279",
      "metadata": {
        "id": "069d0279"
      },
      "outputs": [],
      "source": [
        "def parse_website(soup, url, source):\n",
        "    \"\"\"\n",
        "    Mengurai objek BeautifulSoup dengan selector h1 untuk judul\n",
        "    dan mencoba dua selector untuk konten: div.body-paragraph p atau .deskrip-foto.\n",
        "\n",
        "    Args:\n",
        "        soup (BeautifulSoup object): Objek BeautifulSoup dari halaman yang di-scrape.\n",
        "        url (str): URL artikel.\n",
        "        source (str): Nama sumber website.\n",
        "\n",
        "    Returns:\n",
        "        dict or None: Dictionary data artikel jika berhasil, None jika gagal.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Judul - MENGGUNAKAN SELECTOR h1\n",
        "        judul_tag = soup.select_one(\"h1.read__title\")\n",
        "        if not judul_tag:\n",
        "            print(f\"  ⚠️ Tidak menemukan h1 untuk judul pada {url}\")\n",
        "            return None\n",
        "        judul = judul_tag.text.strip()\n",
        "\n",
        "        # Penulis\n",
        "        penulis_tag = soup.select_one(\".credit-title\")\n",
        "        penulis = penulis_tag.text.strip() if penulis_tag else \"Tidak Ditemukan\"\n",
        "\n",
        "        # Tanggal\n",
        "        tanggal_tag = soup.select_one(\".read__time\")\n",
        "        tanggal_full = tanggal_tag.text.strip() if tanggal_tag else \"Tidak Ditemukan\"\n",
        "\n",
        "        match_date = re.search(r\"(\\d{1,2} \\w+ \\d{4})\", tanggal_full)\n",
        "        match_time = re.search(r\"(\\d{1,2}:\\d{2})\", tanggal_full)\n",
        "\n",
        "        date = match_date.group(1) if match_date else tanggal_full\n",
        "        time_ = match_time.group(1) if match_time else \"\"\n",
        "\n",
        "        # Konten - class=article-content-body__item-content\n",
        "        # Asumsi konten ada di dalam div ini dan mungkin terdiri dari beberapa paragraf atau elemen lain\n",
        "        konten_div = soup.select_one(\".read__content\")\n",
        "        if not konten_div:\n",
        "            print(f\"  ⚠️ Tidak menemukan konten artikel (.read__content) untuk {url}\")\n",
        "            # Anda bisa memilih untuk return None atau dictionary dengan konten kosong\n",
        "            # return None\n",
        "            isi = \"\"\n",
        "        else:\n",
        "            # Ambil semua teks di dalam elemen konten_div\n",
        "            isi = konten_div.get_text(separator=\"\\n\", strip=True)\n",
        "\n",
        "        # Kategori dan Sub-kategori\n",
        "        category = soup.select_one(\".breadcrumb__link\")\n",
        "        category = category.text.strip() if category else \"Tidak Ditemukan\"\n",
        "        sub_category = \"\"\n",
        "\n",
        "        return {\n",
        "            \"title\": judul,\n",
        "            \"source\": source,\n",
        "            \"date\": date,\n",
        "            \"time\": time_,\n",
        "            \"category\": category,\n",
        "            \"sub-category\": sub_category,\n",
        "            \"content\": isi,\n",
        "            \"url\": url,\n",
        "            \"author\": penulis,\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ❌ Gagal mengurai {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# %%\n",
        "def scrape_berita(url, source):\n",
        "    \"\"\"\n",
        "    Mengambil HTML dan memanggil fungsi parse_website (yang spesifik untuk Kapanlagi).\n",
        "\n",
        "    Args:\n",
        "        url (str): URL artikel.\n",
        "        source (str): Nama sumber website.\n",
        "\n",
        "    Returns:\n",
        "        dict or None: Dictionary data artikel jika berhasil di-scrape dan diurai (menggunakan parse_website), None jika gagal.\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\",\n",
        "        \"Referer\": \"https://www.google.com/\",\n",
        "        \"Accept-Language\": \"id,en-US;q=0.9,en;q=0.8\",\n",
        "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
        "    }\n",
        "    try:\n",
        "        resp = requests.get(url, headers=headers, timeout=10)\n",
        "        # Tambahkan pemeriksaan status code\n",
        "        if resp.status_code != 200:\n",
        "            print(f\"  ⚠️ Status code: {resp.status_code} for {url}\")\n",
        "            return None\n",
        "\n",
        "        # Buat objek BeautifulSoup\n",
        "        soup = BeautifulSoup(resp.content, \"html.parser\")\n",
        "\n",
        "        # LANGSUNG PANGGIL FUNGSI parse_website\n",
        "        # INGAT: Fungsi ini dirancang untuk KAPANLAGI.COM dan kemungkinan besar GAGAL untuk situs lain\n",
        "        return parse_website(soup, url, source)\n",
        "\n",
        "    except requests.exceptions.Timeout:\n",
        "        print(f\"  ❌ Timeout saat mengambil {url}\")\n",
        "        return None\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"  ❌ Error saat mengambil {url}: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        # Tangani error lain selama proses\n",
        "        print(f\"  ❌ Gagal memproses HTML dari {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# %%\n",
        "def full_rss_scrape(list_of_rss_urls, output_csv=\"berita.csv\", max_articles=1000):\n",
        "    import xml.etree.ElementTree as ET  # Tambahan untuk sitemap XML\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
        "        \"Referer\": \"https://www.google.com/\",\n",
        "    }\n",
        "\n",
        "    urls = set()\n",
        "    print(\"➡️ Mengumpulkan link dari RSS/Sitemap...\")\n",
        "\n",
        "    for rss_url in list_of_rss_urls:\n",
        "        try:\n",
        "            print(f\"  Mengambil dari: {rss_url}\")\n",
        "            resp = requests.get(rss_url, headers=headers, timeout=10)\n",
        "            content = resp.content.decode(\"utf-8\")\n",
        "\n",
        "            # Cek apakah ini RSS feed biasa atau sitemap XML (berdasarkan root tag)\n",
        "            if \"<rss\" in content or \"<feed\" in content:\n",
        "                # ➤ Tangani sebagai RSS standar\n",
        "                feed = feedparser.parse(content)\n",
        "                for entry in feed.entries:\n",
        "                    article_link = entry.get('link') or entry.get('loc')\n",
        "                    if article_link:\n",
        "                        urls.add(article_link)\n",
        "            elif \"<urlset\" in content:\n",
        "                # ➤ Tangani sebagai Sitemap XML\n",
        "                soup = BeautifulSoup(content, \"xml\")\n",
        "                loc_tags = soup.find_all(\"loc\")\n",
        "                for tag in loc_tags:\n",
        "                    if tag.text.strip():\n",
        "                        urls.add(tag.text.strip())\n",
        "            else:\n",
        "                print(f\"  ⚠️ Tidak dikenali sebagai RSS atau Sitemap: {rss_url}\")\n",
        "\n",
        "            time.sleep(1)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ❌ Gagal ambil dari {rss_url}: {e}\")\n",
        "\n",
        "    urls_to_scrape = list(urls)[:max_articles]\n",
        "    print(f\"✅ Total link unik yang berhasil dikumpulkan: {len(urls)}\")\n",
        "    print(f\"➡️ Akan men-scrape {len(urls_to_scrape)} artikel...\")\n",
        "\n",
        "    def scrape_wrapper(args):\n",
        "        url, source = args\n",
        "        print(f\"  Scraping: {url} | Source: {source}\")\n",
        "        return scrape_berita(url, source)\n",
        "\n",
        "    berita_list = []\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        args_list = [(url, urlparse(url).netloc) for url in urls_to_scrape]\n",
        "        results = list(executor.map(scrape_wrapper, args_list))\n",
        "        for data in results:\n",
        "            if data:\n",
        "                berita_list.append(data)\n",
        "\n",
        "    df = pd.DataFrame(berita_list, columns=[\n",
        "        \"title\", \"source\", \"date\", \"time\", \"category\", \"sub-category\", \"content\", \"url\", \"author\"\n",
        "    ])\n",
        "\n",
        "    print(f\"➡️ Menyimpan hasil ke {output_csv}...\")\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    print(f\"✅ CSV disimpan! Jumlah artikel berhasil di-scrape: {len(df)}\")\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "93525a52",
      "metadata": {
        "id": "93525a52"
      },
      "outputs": [],
      "source": [
        "# Contoh penggunaan fungsi full_rss_scrape:\n",
        "\n",
        "# Ambil daftar link RSS dari file CSV (misalnya hanya untuk 'tribun')\n",
        "dataset = pd.read_csv('https://raw.githubusercontent.com/gikirima/indonews-scrapper/refs/heads/main/link_scrapping.csv')\n",
        "tribun_rss_links = dataset[dataset['website'] == 'tribun']['link'].tolist()\n",
        "sindonews_rss_links = dataset[dataset['website'] == 'sindonews']['link'].tolist()\n",
        "liputan6_rss_links = dataset[dataset['website'] == 'liputan6']['link'].tolist()\n",
        "detik_rss_links = dataset[dataset['website'] == 'detik']['link'].tolist()\n",
        "kapanlagi_rss_links = dataset[dataset['website'] == 'kapanlagi']['link'].tolist()\n",
        "fimela_rss_links = dataset[dataset['website'] == 'fimela']['link'].tolist()\n",
        "okezone_rss_links = dataset[dataset['website'] == 'okezone']['link'].tolist()\n",
        "posmetro_rss_links = dataset[dataset['website'] == 'posmetro']['link'].tolist()\n",
        "kompas_rss_links = dataset[dataset['website'] == 'kompas']['link'].tolist()\n",
        "republika_rss_links = dataset[dataset['website'] == 'republika']['link'].tolist()\n",
        "tempo_rss_links = dataset[dataset['website'] == 'tempo']['link'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "w4MLaEXGrEF9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "w4MLaEXGrEF9",
        "outputId": "84e0b10d-6f25-427f-ff59-5557b7c1cbf8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "➡️ Mengumpulkan link dari RSS/Sitemap...\n",
            "  Mengambil dari: https://rss.kompas.com/api/feed/social?apikey=bc58c81819dff4b8d5c53540a2fc7ffd83e6314a\n",
            "✅ Total link unik yang berhasil dikumpulkan: 50\n",
            "➡️ Akan men-scrape 50 artikel...\n",
            "  Scraping: https://nasional.kompas.com/read/2025/06/10/21233191/pengamat-polisi-yang-lecehkan-korban-pemerkosaan-di-ntt-tak-cukup-diperiksa?utm_source=ABC&utm_medium=rss | Source: nasional.kompas.com\n",
            "  Scraping: https://bola.kompas.com/read/2025/06/10/20281008/hasil-kualifikasi-piala-dunia-2026-jepang-vs-indonesia-6-0-garuda-tetap-ke-ronde?utm_source=ABC&utm_medium=rss | Source: bola.kompas.com\n",
            "  Scraping: https://nasional.kompas.com/read/2025/06/10/21035731/pemerintah-cabut-iup-4-perusahaan-tambang-di-raja-ampat-sekjen-apni-bukan?utm_source=ABC&utm_medium=rss | Source: nasional.kompas.com\n",
            "  Scraping: https://megapolitan.kompas.com/read/2025/06/10/21504001/kemunculan-gangster-remaja-di-bogor-selatan-dinilai-sebagai-perilaku?utm_source=ABC&utm_medium=rss | Source: megapolitan.kompas.com\n",
            "  Scraping: https://megapolitan.kompas.com/read/2025/06/10/20445301/indonesia-dibantai-jepang-6-0-suporter-kami-tetap-bangga?utm_source=ABC&utm_medium=rss | Source: megapolitan.kompas.com\n",
            "  Scraping: https://nasional.kompas.com/read/2025/06/10/20434941/depinas-soksi-dukung-langkah-pemerintah-hentikan-aktivitas-tambang-nikel-di?utm_source=ABC&utm_medium=rss | Source: nasional.kompas.com\n",
            "  Scraping: https://megapolitan.kompas.com/read/2025/06/10/21585621/pramono-pastikan-jakarta-siap-jalankan-putusan-mk-soal-pendidikan-gratis?utm_source=ABC&utm_medium=rss | Source: megapolitan.kompas.com\n",
            "  Scraping: https://nasional.kompas.com/read/2025/06/10/21101201/revisi-uu-pemilu-dan-uu-pilkada-tak-pakai-sistem-omnibus-law?utm_source=ABC&utm_medium=rss | Source: nasional.kompas.com\n",
            "  Scraping: https://nasional.kompas.com/read/2025/06/10/22010331/kemendagri-akan-surati-kepala-daerah-yang-belum-siapkan-lahan-untuk-lokasi?utm_source=ABC&utm_medium=rss | Source: nasional.kompas.com\n",
            "  Scraping: https://money.kompas.com/read/2025/06/10/204300726/bahaya-data-garis-kemiskinan-tak-relevan-jutaan-warga-miskin-tak-tersentuh?utm_source=ABC&utm_medium=rss | Source: money.kompas.com\n",
            "  Scraping: https://bola.kompas.com/read/2025/06/10/22071028/jepang-vs-indonesia-6-0-kluivert-beruntung-skor-tidak-lebih-besar?utm_source=ABC&utm_medium=rss | Source: bola.kompas.com\n",
            "  Scraping: https://nasional.kompas.com/read/2025/06/10/21031671/kejagung-buka-suara-usai-nadiem-bilang-pengadaan-chromebook-didampingi?utm_source=ABC&utm_medium=rss | Source: nasional.kompas.com\n",
            "  Scraping: https://money.kompas.com/read/2025/06/10/202000026/klaim-jaminan-kehilangan-pekerjaan-melonjak-bpjs-watch-sebut-2-faktor-ini-jadi?utm_source=ABC&utm_medium=rss | Source: money.kompas.com\n",
            "  Scraping: https://nasional.kompas.com/read/2025/06/10/20434101/kemensos-resmi-buka-1554-loker-untuk-guru-sekolah-rakyat-ini-syaratnya?utm_source=ABC&utm_medium=rss | Source: nasional.kompas.com\n",
            "  Scraping: https://bola.kompas.com/read/2025/06/10/21315328/patrick-kluivert-timnas-indonesia-sudah-berusaha-jepang-luar-biasa?utm_source=ABC&utm_medium=rss | Source: bola.kompas.com\n",
            "  Scraping: https://nasional.kompas.com/read/2025/06/10/20181871/penjelasan-mendagri-soal-4-pulau-di-aceh-ditetapkan-masuk-sumut?utm_source=ABC&utm_medium=rss | Source: nasional.kompas.com\n",
            "  Scraping: https://megapolitan.kompas.com/read/2025/06/10/22014591/2000-kucing-bakal-disterilisasi-gratis-di-jakarta-timur-tahun-ini?utm_source=ABC&utm_medium=rss | Source: megapolitan.kompas.com\n",
            "  Scraping: https://megapolitan.kompas.com/read/2025/06/10/21130851/pohon-setinggi-11-meter-tumbang-di-jalan-ciater-raya-tangsel?utm_source=ABC&utm_medium=rss | Source: megapolitan.kompas.com\n",
            "  Scraping: https://nasional.kompas.com/read/2025/06/10/20382871/nadiem-pastikan-tak-ada-monopoli-dalam-proses-pengadaan-laptop-chromebook?utm_source=ABC&utm_medium=rss | Source: nasional.kompas.com\n",
            "  Scraping: https://nasional.kompas.com/read/2025/06/10/21085821/kumpulkan-rosan-hingga-tito-prabowo-bahas-pembangunan-tanggul-laut-di-pulau?utm_source=ABC&utm_medium=rss | Source: nasional.kompas.com\n",
            "  Scraping: https://megapolitan.kompas.com/read/2025/06/10/21192671/polisi-sebut-gangster-dulis-di-bogor-belum-pernah-terlibat-tawuran?utm_source=ABC&utm_medium=rss | Source: megapolitan.kompas.com\n",
            "  Scraping: https://nasional.kompas.com/read/2025/06/10/22025071/jamdatun-rekomendasikan-laptop-windows-ke-nadiem-tetapi-yang-dibeli?utm_source=ABC&utm_medium=rss | Source: nasional.kompas.com\n",
            "  Scraping: https://nasional.kompas.com/read/2025/06/10/20421881/pks-ingin-temui-prabowo-sampaikan-komitmen-dukung-pemerintah?utm_source=ABC&utm_medium=rss | Source: nasional.kompas.com\n",
            "  Scraping: https://nasional.kompas.com/read/2025/06/10/22054011/kebijakan-baru-haji-2026-kepala-bp-haji-maksimal-2-syarikah?utm_source=ABC&utm_medium=rss | Source: nasional.kompas.com\n",
            "  Scraping: https://nasional.kompas.com/read/2025/06/10/21302471/hampir-12-jam-bos-sritex-diperiksa-kejagung-soal-korupsi-pemberian-kredit?utm_source=ABC&utm_medium=rss | Source: nasional.kompas.com\n",
            "  Scraping: https://travel.kompas.com/read/2025/06/10/201800927/gratis-masuk-ancol-pada-10-20-juni-2025-ini-syaratnya?utm_source=ABC&utm_medium=rss | Source: travel.kompas.com\n",
            "  Scraping: https://nasional.kompas.com/read/2025/06/10/21485591/bos-sritex-masih-tunggu-panggilan-pemeriksaan-lanjutan-kejagung?utm_source=ABC&utm_medium=rss | Source: nasional.kompas.com\n",
            "  Scraping: https://nasional.kompas.com/read/2025/06/10/21441991/jatam-minta-pemerintah-juga-setop-tambang-di-pulau-gag-raja-ampat?utm_source=ABC&utm_medium=rss | Source: nasional.kompas.com\n",
            "  Scraping: https://nasional.kompas.com/read/2025/06/10/20551111/danantara-akan-dilibatkan-dalam-proyek-pengelolaan-sampah-waste-to-energy?utm_source=ABC&utm_medium=rss | Source: nasional.kompas.com\n",
            "  Scraping: https://megapolitan.kompas.com/read/2025/06/10/21023741/soal-wacana-car-free-night-jakarta-pramono-kami-kaji-lebih-dulu?utm_source=ABC&utm_medium=rss | Source: megapolitan.kompas.com\n",
            "  Scraping: https://megapolitan.kompas.com/read/2025/06/10/21243621/pasar-baru-mati-suri-rano-karno-ruh-jakarta-yang-terlupakan?utm_source=ABC&utm_medium=rss | Source: megapolitan.kompas.com\n",
            "  Scraping: https://megapolitan.kompas.com/read/2025/06/10/21083721/kebakaran-di-cengkareng-diduga-dipicu-pemotongan-rangka-bus-untuk-dijual?utm_source=ABC&utm_medium=rss | Source: megapolitan.kompas.com\n",
            "  Scraping: https://megapolitan.kompas.com/read/2025/06/10/20220221/keluhkan-mesin-pembakar-sampah-di-tps3r-warga-poris-indah-asapnya?utm_source=ABC&utm_medium=rss | Source: megapolitan.kompas.com\n",
            "  Scraping: https://nasional.kompas.com/read/2025/06/10/20235821/korupsi-pengadaan-lahan-jaksa-tuntut-eks-dirut-sarana-jaya-55-tahun-bui?utm_source=ABC&utm_medium=rss | Source: nasional.kompas.com\n",
            "  Scraping: https://megapolitan.kompas.com/read/2025/06/10/21315781/hakim-pn-jaksel-pertanyakan-kinerja-2-eks-direktur-komdigi-soal?utm_source=ABC&utm_medium=rss | Source: megapolitan.kompas.com\n",
            "  Scraping: https://megapolitan.kompas.com/read/2025/06/10/21401421/timnas-indonesia-dibantai-jepang-suporter-sudah-pesimistis-dari-awal?utm_source=ABC&utm_medium=rss | Source: megapolitan.kompas.com\n",
            "  Scraping: https://money.kompas.com/read/2025/06/10/210300626/menurut-menteri-maman-ini-kriteria-umkm-yang-bisa-kelola-tambang?utm_source=ABC&utm_medium=rss | Source: money.kompas.com\n",
            "  Scraping: https://bola.kompas.com/read/2025/06/10/21585328/kluivert-jelaskan-sulitnya-timnas-indonesia-bertahan-melawan-jepang?utm_source=ABC&utm_medium=rss | Source: bola.kompas.com\n",
            "  Scraping: https://money.kompas.com/read/2025/06/10/201739426/hipmi-dukung-bahlil-sigap-tangani-polemik-tambang-di-raja-ampat?utm_source=ABC&utm_medium=rss | Source: money.kompas.com\n",
            "  Scraping: https://nasional.kompas.com/read/2025/06/10/22075251/nadiem-mengaku-siap-klarifikasi-soal-korupsi-chromebook-ini-kata-kejagung?utm_source=ABC&utm_medium=rss | Source: nasional.kompas.com\n",
            "  Scraping: https://megapolitan.kompas.com/read/2025/06/10/21530301/akses-jalan-ciater-raya-sempat-lumpuh-akibat-pohon-tumbang?utm_source=ABC&utm_medium=rss | Source: megapolitan.kompas.com\n",
            "  Scraping: https://nasional.kompas.com/read/2025/06/10/20532191/kepala-bp-haji-sebut-ada-wacana-kuota-haji-2026-dikurangi-50-persen?utm_source=ABC&utm_medium=rss | Source: nasional.kompas.com\n",
            "  Scraping: https://megapolitan.kompas.com/read/2025/06/10/20404031/denden-imadudin-aktif-jadi-saksi-ahli-judol-dalam-persidangan?utm_source=ABC&utm_medium=rss | Source: megapolitan.kompas.com\n",
            "  Scraping: https://nasional.kompas.com/read/2025/06/10/21170841/rekrutmen-guru-untuk-sekolah-rakyat-resmi-dibuka-berikut-kualifikasinya?utm_source=ABC&utm_medium=rss | Source: nasional.kompas.com\n",
            "  Scraping: https://money.kompas.com/read/2025/06/10/202514926/bni-dukung-umkm-naik-kelas-dan-go-global?utm_source=ABC&utm_medium=rss | Source: money.kompas.com\n",
            "  Scraping: https://megapolitan.kompas.com/read/2025/06/10/21472491/tawuran-di-pasar-rebo-disebabkan-saling-tantang-di-medsos?utm_source=ABC&utm_medium=rss | Source: megapolitan.kompas.com\n",
            "  Scraping: https://lifestyle.kompas.com/read/2025/06/10/203100020/6-penyebab-seseorang-melakukan-ghosting-dalam-hubungan-takut-komitmen?utm_source=ABC&utm_medium=rss | Source: lifestyle.kompas.com\n",
            "  Scraping: https://megapolitan.kompas.com/read/2025/06/10/21315951/50-bangkai-bus-transjakarta-terbakar-di-cengkareng-diduga-akibat-percikan?utm_source=ABC&utm_medium=rss | Source: megapolitan.kompas.com\n",
            "  Scraping: https://money.kompas.com/read/2025/06/10/214722226/menteri-yusril-pertumbuhan-ekonomi-indonesia-kalah-dari-singapura-karena?utm_source=ABC&utm_medium=rss | Source: money.kompas.com\n",
            "  Scraping: https://bola.kompas.com/read/2025/06/10/21000958/jepang-vs-indonesia-6-0-kluivert-dapat-pelajaran-dari-tim-level-piala-dunia?utm_source=ABC&utm_medium=rss | Source: bola.kompas.com\n",
            "➡️ Menyimpan hasil ke berita_kompas.csv...\n",
            "✅ CSV disimpan! Jumlah artikel berhasil di-scrape: 50\n"
          ]
        }
      ],
      "source": [
        "# df_sindonews = full_rss_scrape(sindonews_rss_links, output_csv=\"berita_sindonews.csv\", max_articles=500)\n",
        "# df_liputan6 = full_rss_scrape(liputan6_rss_links, output_csv=\"berita_liputan6.csv\", max_articles=500)\n",
        "# df_detik = full_rss_scrape(detik_rss_links, output_csv=\"berita_detik\", max_articles=1000)\n",
        "# df_kapanlagi = full_rss_scrape(kapanlagi_rss_links, output_csv=\"berita_kapanlagi.csv\", max_articles=500)\n",
        "# df_fimela = full_rss_scrape(fimela_rss_links, output_csv=\"berita_fimela.csv\", max_articles=6000)\n",
        "# df_okezone = full_rss_scrape(okezone_rss_links, output_csv=\"berita_okezone.csv\", max_articles=500)\n",
        "# df_posmetro = full_rss_scrape(posmetro_rss_links, output_csv=\"berita_posmetro.csv\", max_articles=500)\n",
        "df_kompas = full_rss_scrape(kompas_rss_links, output_csv=\"berita_kompas.csv\", max_articles=1000)\n",
        "# df_republika = full_rss_scrape(republika_rss_links, output_csv=\"berita_republika.csv\", max_articles=500)\n",
        "# df_tempo = full_rss_scrape(tempo_rss_links, output_csv=\"berita_tempo.csv\", max_articles=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XRom_oSd5QzE",
      "metadata": {
        "id": "XRom_oSd5QzE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
