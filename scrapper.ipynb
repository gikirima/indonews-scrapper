{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f73590a5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f73590a5",
        "outputId": "f7f7c0ea-ffed-4104-a80d-184b6b434c11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting sgmllib3k (from feedparser)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=45d31e891dad1c5cf7290833f7bf6d0fc910719e2235846e7dc2eb29ed392ad9\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser\n",
            "Successfully installed feedparser-6.0.11 sgmllib3k-1.0.0\n"
          ]
        }
      ],
      "source": [
        "%pip install feedparser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b494b56a",
      "metadata": {
        "id": "b494b56a"
      },
      "outputs": [],
      "source": [
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "from urllib.parse import urlparse\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import feedparser\n",
        "import pandas as pd\n",
        "import time\n",
        "import re\n",
        "import concurrent.futures\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "069d0279",
      "metadata": {
        "id": "069d0279"
      },
      "outputs": [],
      "source": [
        "# %%\n",
        "def scrape_berita(url, source):\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\",\n",
        "        \"Referer\": \"https://www.google.com/\",\n",
        "        \"Accept-Language\": \"id,en-US;q=0.9,en;q=0.8\",\n",
        "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
        "    }\n",
        "    try:\n",
        "        resp = requests.get(url, headers=headers, timeout=10)\n",
        "        if resp.status_code != 200:\n",
        "            print(f\"  ⚠️ Status code: {resp.status_code} for {url}\")\n",
        "            return None\n",
        "        soup = BeautifulSoup(resp.content, \"html.parser\")\n",
        "\n",
        "        judul = \"\"\n",
        "        date = \"\"\n",
        "        time_ = \"\"\n",
        "        category = \"\"\n",
        "        sub_category = \"\"\n",
        "        isi = \"\"\n",
        "\n",
        "        # Logika scraping berdasarkan sumber website\n",
        "        if \"sindonews.com\" in source:\n",
        "            # Untuk sindonews.com\n",
        "            judul_tag = soup.select_one(\".detail-title\")\n",
        "            judul = judul_tag.text.strip() if judul_tag else \"Tidak ditemukan\"\n",
        "\n",
        "            tanggal_tag = soup.select_one(\".detail-date-artikel\")\n",
        "            tanggal_full = tanggal_tag.text.strip() if tanggal_tag else \"Tidak ditemukan\"\n",
        "            match = re.search(r\"(\\d{1,2}/\\d{1,2}/\\d{4})[ ,]*(\\d{1,2}:\\d{2})?\", tanggal_full)\n",
        "            date = match.group(1) if match else tanggal_full\n",
        "            time_ = match.group(2) if match and match.group(2) else \"\"\n",
        "\n",
        "            # Konten - sindonews.com menggunakan ID\n",
        "            isi_div = soup.select_one(\"#detail-desc\")\n",
        "            isi = \"\\n\".join([p.text.strip() for p in isi_div.find_all(\"p\") if p.text.strip()]) if isi_div else \"Tidak ditemukan\"\n",
        "\n",
        "            # Kategori (Anda sebutkan Bola untuk contoh)\n",
        "            # Perlu dicari selector yang lebih umum jika ingin otomatis\n",
        "            category = \"Bola\" # Contoh tetap\n",
        "\n",
        "            # Sub-kategori (Perlu dicari selector yang sesuai)\n",
        "            sub_category = \"\" # Contoh tetap\n",
        "\n",
        "        elif \"tribunnews.com\" in source:\n",
        "            # Untuk tribunnews.com (logika scraping yang sudah ada)\n",
        "            judul_tag = soup.select_one(\"h1\")\n",
        "            judul = judul_tag.text.strip() if judul_tag else \"Tidak ditemukan\"\n",
        "\n",
        "            isi_paragraf = soup.select(\".side-article p\")\n",
        "            isi = \"\\n\".join([p.text.strip() for p in isi_paragraf if p.text.strip()]) if isi_paragraf else \"Tidak ditemukan\"\n",
        "\n",
        "            # Tanggal dan waktu\n",
        "            tanggal_tag = soup.select_one(\"time\") or soup.select_one(\".date\")\n",
        "            tanggal_full = tanggal_tag.text.strip() if tanggal_tag else \"Tidak ditemukan\"\n",
        "            match = re.search(r\"(\\d{1,2}/\\d{1,2}/\\d{4})[ ,]*(\\d{1,2}:\\d{2})?\", tanggal_full)\n",
        "            date = match.group(1) if match else tanggal_full\n",
        "            time_ = match.group(2) if match and match.group(2) else \"\"\n",
        "\n",
        "            # Breadcrumb: kategori dan sub-kategori\n",
        "            breadcrumb = soup.select(\".breadcrumb li a\")\n",
        "            category = breadcrumb[1].text.strip() if len(breadcrumb) > 1 else \"\"\n",
        "            sub_category = breadcrumb[2].text.strip() if len(breadcrumb) > 2 else \"\"\n",
        "\n",
        "        # Tambahkan elif untuk website lain di sini\n",
        "        # elif \"namadomainlain.com\" in source:\n",
        "        #    ... logika scraping untuk namadomainlain.com ...\n",
        "\n",
        "        else:\n",
        "            # Logika default atau pesan jika sumber tidak dikenal\n",
        "            print(f\"  ⚠️ Sumber tidak dikenali untuk scraping: {source}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "        # Pastikan semua field terisi (minimal dengan string kosong)\n",
        "        return {\n",
        "            \"title\": judul if judul else \"\",\n",
        "            \"source\": source if source else \"\",\n",
        "            \"date\": date if date else \"\",\n",
        "            \"time\": time_ if time_ else \"\",\n",
        "            \"category\": category if category else \"\",\n",
        "            \"sub-category\": sub_category if sub_category else \"\",\n",
        "            \"content\": isi if isi else \"\",\n",
        "            \"url\": url if url else \"\",\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ❌ Gagal scraping {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Sisa kode fungsi full_rss_scrape dan contoh penggunaan tetap sama\n",
        "# %%\n",
        "def full_rss_scrape(list_of_rss_urls, output_csv=\"berita.csv\", max_articles=1000):\n",
        "    \"\"\"\n",
        "    Mengambil link artikel dari daftar URL RSS, kemudian melakukan scraping,\n",
        "    dan menyimpan hasilnya ke file CSV.\n",
        "\n",
        "    Args:\n",
        "        list_of_rss_urls (list): Daftar URL RSS yang akan diambil link-nya.\n",
        "        output_csv (str): Nama file CSV untuk menyimpan hasil scraping.\n",
        "        max_articles (int): Jumlah maksimum artikel yang akan di-scrape dari link RSS.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: DataFrame yang berisi data artikel yang berhasil di-scrape.\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
        "        \"Referer\": \"https://www.google.com/\",\n",
        "    }\n",
        "\n",
        "    urls = set()\n",
        "    print(\"➡️ Mengumpulkan link dari RSS...\")\n",
        "    for rss_url in list_of_rss_urls:\n",
        "        try:\n",
        "            print(f\"  Mengambil dari: {rss_url}\")\n",
        "            resp = requests.get(rss_url, headers=headers, timeout=10)\n",
        "            feed = feedparser.parse(resp.text)\n",
        "            for entry in feed.entries:\n",
        "                # Hanya tambahkan jika link tidak kosong\n",
        "                if entry.link:\n",
        "                  urls.add(entry.link)\n",
        "            time.sleep(1) # Memberi jeda agar tidak terlalu cepat\n",
        "        except Exception as e:\n",
        "            print(f\"  ❌ Gagal ambil dari {rss_url}: {e}\")\n",
        "\n",
        "    # Ambil sejumlah URL sesuai parameter max_articles\n",
        "    urls_to_scrape = list(urls)[:max_articles]\n",
        "    print(f\"✅ Total link unik yang berhasil dikumpulkan: {len(urls)}\")\n",
        "    print(f\"➡️ Akan men-scrape {len(urls_to_scrape)} artikel...\")\n",
        "\n",
        "    print(\"➡️ Memulai proses scraping artikel...\")\n",
        "    def scrape_wrapper(args):\n",
        "        url, source = args\n",
        "        print(f\"  Scraping: {url} | Source: {source}\")\n",
        "        return scrape_berita(url, source)\n",
        "\n",
        "    berita_list = []\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        # Menggunakan urlparse untuk mendapatkan source (nama domain)\n",
        "        args_list = [(url, urlparse(url).netloc) for url in urls_to_scrape]\n",
        "        results = list(executor.map(scrape_wrapper, args_list))\n",
        "        for data in results:\n",
        "            if data:\n",
        "                berita_list.append(data)\n",
        "\n",
        "    df = pd.DataFrame(berita_list, columns=[\n",
        "        \"title\", \"source\", \"date\", \"time\", \"category\", \"sub-category\", \"content\", \"url\"\n",
        "    ])\n",
        "\n",
        "    # Simpan hasil ke CSV\n",
        "    print(f\"➡️ Menyimpan hasil ke {output_csv}...\")\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    print(f\"✅ CSV disimpan! Jumlah artikel berhasil di-scrape: {len(df)}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# %%\n",
        "# Contoh penggunaan fungsi full_rss_scrape:\n",
        "\n",
        "# Ambil daftar link RSS dari file CSV (misalnya hanya untuk 'tribun')\n",
        "dataset = pd.read_csv('https://raw.githubusercontent.com/gikirima/indonews-scrapper/refs/heads/main/link_scrapping.csv')\n",
        "tribun_rss_links = dataset[dataset['website'] == 'tribun']['link'].tolist()\n",
        "sindonews_rss_links = dataset[dataset['website'] == 'sindonews']['link'].tolist()\n",
        "liputan6_rss_links = dataset[dataset['website'] == 'liputan6']['link'].tolist()\n",
        "detik_rss_links = dataset[dataset['website'] == 'detik']['link'].tolist()\n",
        "kapanlagi_rss_links = dataset[dataset['website'] == 'kapanlagi']['link'].tolist()\n",
        "fimela_rss_links = dataset[dataset['website'] == 'fimela']['link'].tolist()\n",
        "okezone_rss_links = dataset[dataset['website'] == 'okezone']['link'].tolist()\n",
        "posmetro_rss_links = dataset[dataset['website'] == 'posmetro']['link'].tolist()\n",
        "kompas_rss_links = dataset[dataset['website'] == 'kompas']['link'].tolist()\n",
        "republika_rss_links = dataset[dataset['website'] == 'republika']['link'].tolist()\n",
        "tempo_rss_links = dataset[dataset['website'] == 'tempo']['link'].tolist()\n",
        "\n",
        "\n",
        "# %%\n",
        "# Panggil fungsi full_rss_scrape dengan daftar link RSS\n",
        "# Anda bisa menyesuaikan nama file output CSV dan jumlah maksimum artikel\n",
        "\n",
        "# Contoh untuk sindonews\n",
        "print(\"Scraping Sindonews...\")\n",
        "df_sindonews = full_rss_scrape(sindonews_rss_links, output_csv=\"berita_sindonews.csv\", max_articles=10) # Ubah max_articles untuk pengujian\n",
        "\n",
        "# Contoh untuk tribunnews (menggunakan logika yang sudah ada)\n",
        "print(\"\\nScraping Tribunnews...\")\n",
        "df_tribun = full_rss_scrape(tribun_rss_links, output_csv=\"berita_tribun.csv\", max_articles=10) # Ubah max_articles untuk pengujian\n",
        "\n",
        "\n",
        "# Jika Anda ingin melihat DataFrame hasilnya:\n",
        "display(df_sindonews.head())\n",
        "display(df_tribun.head())\n",
        "# %%\n",
        "# Anda bisa memanggil fungsi untuk website lain di sini\n",
        "# df_liputan6 = full_rss_scrape(liputan6_rss_links, output_csv=\"berita_liputan6.csv\", max_articles=500)\n",
        "# ... dan seterusnya\n",
        "\n",
        "Use code with caution\n",
        "\n",
        "Penjelasan Perubahan:\n",
        "\n",
        "    Parameter source di scrape_berita: Fungsi scrape_berita sekarang menerima parameter source yang berisi nama domain dari URL artikel. Parameter ini didapatkan dari fungsi full_rss_scrape menggunakan urlparse(url).netloc.\n",
        "    Kondisi if-elif-else: Di awal fungsi scrape_berita, ditambahkan kondisi if \"sindonews.com\" in source: untuk mengecek apakah URL berasal dari sindonews.com.\n",
        "    Logika Scraping Khusus Sindonews: Di dalam blok if untuk sindonews, digunakan selector CSS yang Anda berikan (.detail-title untuk judul, .detail-date-artikel untuk tanggal, dan #detail-desc untuk konten). XPath (//*[@id=\"detail-desc\"]) tidak digunakan langsung di BeautifulSoup; sebagai gantinya, kita menggunakan selector CSS (#detail-desc) dan kemudian mencari paragraf (<p>) di dalamnya. Untuk kategori, saat ini diatur tetap \"Bola\" sesuai contoh Anda; Anda perlu menyesuaikannya jika ingin mengekstrak kategori secara dinamis dari halaman.\n",
        "    Logika Scraping Tribunnews: Di blok elif \"tribunnews.com\" in source:, logika scraping yang sudah ada sebelumnya untuk tribunnews dipindahkan.\n",
        "    Penanganan Sumber Lain: Anda bisa menambahkan blok elif baru untuk setiap website lain yang ingin Anda dukung, dengan menyediakan selector CSS atau metode BeautifulSoup yang sesuai untuk mengekstrak data dari struktur HTML mereka.\n",
        "    Logika Default: Blok else ditambahkan untuk menangani URL dari sumber yang tidak dikenali oleh fungsi, mencetak pesan peringatan dan mengembalikan None.\n",
        "    Penanganan Hasil None: Fungsi full_rss_scrape tetap menangani hasil None dari scrape_berita dengan tidak menambahkannya ke berita_list.\n",
        "    Memastikan Field Terisi: Sebelum mengembalikan dictionary hasil scraping, ditambahkan pengecekan sederhana untuk memastikan setiap field memiliki nilai (setidaknya string kosong) untuk menghindari error saat membuat DataFrame.\n",
        "    Perubahan di full_rss_scrape: Di dalam full_rss_scrape, saat memanggil scrape_wrapper (dan pada akhirnya scrape_berita), parameter source sekarang dikirimkan menggunakan urlparse(url).netloc.\n",
        "\n",
        "Dengan perubahan ini, fungsi scrape_berita sekarang bisa secara otomatis menyesuaikan logika scraping berdasarkan website asal artikel. Anda hanya perlu menambahkan blok elif baru untuk setiap website lain yang ingin Anda dukung dan menentukan selector yang tepat untuk masing-masing website tersebut.\n",
        "\n",
        "Ingatlah bahwa selector HTML bisa berubah sewaktu-waktu, jadi mungkin Anda perlu menyesuaikan kode ini jika struktur halaman website berubah.\n",
        "\n",
        "itu aku udah ubah fungsinya menjadi parse_tribun untuk tribun news. coba kamu tambahkan parse_sindonews seperti yang telah kubuat di atas. untuk judul kan class=detail-title, date class=detail-date-artikel, dan content class=detail-desc.\n",
        "\n",
        "coba kamu buat parse nya\n",
        "\n",
        "Baik, mari kita modifikasi kode Anda untuk memisahkan logika parsing untuk Tribunnews dan Sindonews ke dalam fungsi terpisah (parse_tribun dan parse_sindonews), dan kemudian menggunakan kondisi if-else di fungsi utama (misalnya, kita buat fungsi parse_artikel) untuk memanggil fungsi parsing yang sesuai.\n",
        "\n",
        "Ini akan membuat kode lebih modular dan mudah dikelola untuk setiap jenis website.\n",
        "\n",
        "Berikut langkah-langkahnya:\n",
        "\n",
        "    Fungsi parse_tribun: Ini adalah fungsi yang sudah Anda miliki untuk Tribunnews. Kita akan tetap menggunakannya.\n",
        "    Fungsi parse_sindonews: Kita akan membuat fungsi baru khusus untuk mengurai halaman Sindonews.\n",
        "    Fungsi parse_artikel (atau nama lain): Fungsi ini akan menerima URL dan nama sumber, kemudian memanggil parse_tribun atau parse_sindonews berdasarkan sumbernya.\n",
        "    Modifikasi full_rss_scrape: Fungsi ini akan memanggil parse_artikel (atau fungsi baru yang dibuat di langkah 3) alih-alih memanggil scrape_berita secara langsung (atau kita bisa menamai fungsi baru tersebut scrape_berita).\n",
        "\n",
        "Mari kita implementasikan:\n",
        "\n",
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "from urllib.parse import urlparse\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import feedparser\n",
        "import pandas as pd\n",
        "import time\n",
        "import re\n",
        "import concurrent.futures\n",
        "\n",
        "# %%\n",
        "%pip install feedparser\n",
        "# %%\n",
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "from urllib.parse import urlparse\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import feedparser\n",
        "import pandas as pd\n",
        "import time\n",
        "import re\n",
        "import concurrent.futures\n",
        "\n",
        "# %%\n",
        "def parse_tribun(soup, url, source):\n",
        "    \"\"\"\n",
        "    Mengurai objek BeautifulSoup untuk website tribunnews.com.\n",
        "\n",
        "    Args:\n",
        "        soup (BeautifulSoup object): Objek BeautifulSoup dari halaman yang di-scrape.\n",
        "        url (str): URL artikel.\n",
        "        source (str): Nama sumber website.\n",
        "\n",
        "    Returns:\n",
        "        dict or None: Dictionary data artikel jika berhasil, None jika gagal.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        judul_tag = soup.select_one(\"h1\")\n",
        "        if not judul_tag:\n",
        "            print(\"  ⚠️ Tidak menemukan tag <h1> untuk tribunnews.com\")\n",
        "            return None\n",
        "\n",
        "        isi_paragraf = soup.select(\".side-article p\")\n",
        "        if not isi_paragraf:\n",
        "            print(\"  ⚠️ Tidak menemukan isi artikel untuk tribunnews.com\")\n",
        "            return None\n",
        "\n",
        "        judul = judul_tag.text.strip()\n",
        "        isi = \"\\n\".join([p.text.strip() for p in isi_paragraf if p.text.strip()])\n",
        "\n",
        "        # Tanggal dan waktu\n",
        "        tanggal_tag = soup.select_one(\"time\") or soup.select_one(\".date\")\n",
        "        tanggal_full = tanggal_tag.text.strip() if tanggal_tag else \"Tidak ditemukan\"\n",
        "        match = re.search(r\"(\\d{1,2}/\\d{1,2}/\\d{4})[ ,]*(\\d{1,2}:\\d{2})?\", tanggal_full)\n",
        "        date = match.group(1) if match else tanggal_full\n",
        "        time_ = match.group(2) if match and match.group(2) else \"\"\n",
        "\n",
        "        # Breadcrumb: kategori dan sub-kategori\n",
        "        breadcrumb = soup.select(\".breadcrumb li a\")\n",
        "        category = breadcrumb[1].text.strip() if len(breadcrumb) > 1 else \"\"\n",
        "        sub_category = breadcrumb[2].text.strip() if len(breadcrumb) > 2 else \"\"\n",
        "\n",
        "        return {\n",
        "            \"title\": judul,\n",
        "            \"source\": source,\n",
        "            \"date\": date,\n",
        "            \"time\": time_,\n",
        "            \"category\": category,\n",
        "            \"sub-category\": sub_category,\n",
        "            \"content\": isi,\n",
        "            \"url\": url,\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ❌ Gagal mengurai tribunnews.com {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# %%\n",
        "def parse_sindonews(soup, url, source):\n",
        "    \"\"\"\n",
        "    Mengurai objek BeautifulSoup untuk website sindonews.com.\n",
        "\n",
        "    Args:\n",
        "        soup (BeautifulSoup object): Objek BeautifulSoup dari halaman yang di-scrape.\n",
        "        url (str): URL artikel.\n",
        "        source (str): Nama sumber website.\n",
        "\n",
        "    Returns:\n",
        "        dict or None: Dictionary data artikel jika berhasil, None jika gagal.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        judul_tag = soup.select_one(\".detail-title\")\n",
        "        if not judul_tag:\n",
        "             print(\"  ⚠️ Tidak menemukan .detail-title untuk sindonews.com\")\n",
        "             return None\n",
        "\n",
        "        tanggal_tag = soup.select_one(\".detail-date-artikel\")\n",
        "        if not tanggal_tag:\n",
        "            print(\"  ⚠️ Tidak menemukan .detail-date-artikel untuk sindonews.com\")\n",
        "            # Masih bisa lanjut tanpa tanggal/waktu, beri nilai default\n",
        "            tanggal_full = \"Tidak ditemukan\"\n",
        "        else:\n",
        "            tanggal_full = tanggal_tag.text.strip()\n",
        "\n",
        "\n",
        "        isi_div = soup.select_one(\"#detail-desc\")\n",
        "        if not isi_div:\n",
        "            print(\"  ⚠️ Tidak menemukan #detail-desc untuk sindonews.com\")\n",
        "            return None # Konten adalah elemen penting\n",
        "\n",
        "        judul = judul_tag.text.strip()\n",
        "\n",
        "        # Mengambil teks dari semua paragraf di dalam div #detail-desc\n",
        "        isi = \"\\n\".join([p.text.strip() for p in isi_div.find_all(\"p\") if p.text.strip()])\n",
        "\n",
        "        # Parsing tanggal dan waktu\n",
        "        match = re.search(r\"(\\d{1,2}/\\d{1,2}/\\d{4})[ ,]*(\\d{1,2}:\\d{2})?\", tanggal_full)\n",
        "        date = match.group(1) if match else tanggal_full\n",
        "        time_ = match.group(2) if match and match.group(2) else \"\"\n",
        "\n",
        "        # Kategori dan Sub-kategori (Perlu dicari selector yang sesuai di Sindonews)\n",
        "        # Untuk contoh, kita bisa coba cari di breadcrumb atau meta tag\n",
        "        # Jika sulit ditemukan selector umum, mungkin perlu diatur manual atau dibiarkan kosong.\n",
        "        # Contoh: mencari di breadcrumb\n",
        "        breadcrumb = soup.select(\".breadcrumb li a\") # Coba selector umum, mungkin beda\n",
        "        category = breadcrumb[1].text.strip() if len(breadcrumb) > 1 else \"\"\n",
        "        sub_category = breadcrumb[2].text.strip() if len(breadcrumb) > 2 else \"\"\n",
        "\n",
        "        # Jika breadcrumb di atas tidak bekerja, bisa coba meta tag atau bagian lain di HTML\n",
        "        # meta_category = soup.select_one('meta[property=\"article:section\"]')\n",
        "        # category = meta_category['content'] if meta_category and 'content' in meta_category.attrs else \"\"\n",
        "\n",
        "        return {\n",
        "            \"title\": judul,\n",
        "            \"source\": source,\n",
        "            \"date\": date,\n",
        "            \"time\": time_,\n",
        "            \"category\": category, # Mungkin perlu disesuaikan\n",
        "            \"sub-category\": sub_category, # Mungkin perlu disesuaikan\n",
        "            \"content\": isi,\n",
        "            \"url\": url,\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ❌ Gagal mengurai sindonews.com {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# %%\n",
        "def scrape_berita(url, source):\n",
        "    \"\"\"\n",
        "    Mengambil HTML dan memanggil fungsi parsing yang sesuai berdasarkan sumber.\n",
        "\n",
        "    Args:\n",
        "        url (str): URL artikel.\n",
        "        source (str): Nama sumber website.\n",
        "\n",
        "    Returns:\n",
        "        dict or None: Dictionary data artikel jika berhasil di-scrape dan diurai, None jika gagal.\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\",\n",
        "        \"Referer\": \"https://www.google.com/\",\n",
        "        \"Accept-Language\": \"id,en-US;q=0.9,en;q=0.8\",\n",
        "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
        "    }\n",
        "    try:\n",
        "        resp = requests.get(url, headers=headers, timeout=10)\n",
        "        if resp.status_code != 200:\n",
        "            print(f\"  ⚠️ Status code: {resp.status_code} for {url}\")\n",
        "            return None\n",
        "        soup = BeautifulSoup(resp.content, \"html.parser\")\n",
        "\n",
        "        # Pilih fungsi parsing berdasarkan sumber\n",
        "        if \"tribunnews.com\" in source:\n",
        "            return parse_tribun(soup, url, source)\n",
        "        elif \"sindonews.com\" in source:\n",
        "            return parse_sindonews(soup, url, source)\n",
        "        # Tambahkan elif untuk website lain di sini\n",
        "        # elif \"namadomainlain.com\" in source:\n",
        "        #    return parse_namadomainlain(soup, url, source)\n",
        "        else:\n",
        "            print(f\"  ⚠️ Sumber tidak dikenali untuk parsing: {source}\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ❌ Gagal mengambil atau memproses HTML dari {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# %%\n",
        "def full_rss_scrape(list_of_rss_urls, output_csv=\"berita.csv\", max_articles=1000):\n",
        "    \"\"\"\n",
        "    Mengambil link artikel dari daftar URL RSS, kemudian melakukan scraping,\n",
        "    dan menyimpan hasilnya ke file CSV.\n",
        "\n",
        "    Args:\n",
        "        list_of_rss_urls (list): Daftar URL RSS yang akan diambil link-nya.\n",
        "        output_csv (str): Nama file CSV untuk menyimpan hasil scraping.\n",
        "        max_articles (int): Jumlah maksimum artikel yang akan di-scrape dari link RSS.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: DataFrame yang berisi data artikel yang berhasil di-scrape.\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
        "        \"Referer\": \"https://www.google.com/\",\n",
        "    }\n",
        "\n",
        "    urls = set()\n",
        "    print(\"➡️ Mengumpulkan link dari RSS...\")\n",
        "    for rss_url in list_of_rss_urls:\n",
        "        try:\n",
        "            print(f\"  Mengambil dari: {rss_url}\")\n",
        "            resp = requests.get(rss_url, headers=headers, timeout=10)\n",
        "            feed = feedparser.parse(resp.text)\n",
        "            for entry in feed.entries:\n",
        "                # Hanya tambahkan jika link tidak kosong\n",
        "                if entry.link:\n",
        "                  urls.add(entry.link)\n",
        "            time.sleep(1) # Memberi jeda agar tidak terlalu cepat\n",
        "        except Exception as e:\n",
        "            print(f\"  ❌ Gagal ambil dari {rss_url}: {e}\")\n",
        "\n",
        "    # Ambil sejumlah URL sesuai parameter max_articles\n",
        "    urls_to_scrape = list(urls)[:max_articles]\n",
        "    print(f\"✅ Total link unik yang berhasil dikumpulkan: {len(urls)}\")\n",
        "    print(f\"➡️ Akan men-scrape {len(urls_to_scrape)} artikel...\")\n",
        "\n",
        "    print(\"➡️ Memulai proses scraping artikel...\")\n",
        "    def scrape_wrapper(args):\n",
        "        url, source = args\n",
        "        print(f\"  Scraping: {url} | Source: {source}\")\n",
        "        # Memanggil fungsi scrape_berita yang sudah diperbarui\n",
        "        return scrape_berita(url, source)\n",
        "\n",
        "    berita_list = []\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        # Menggunakan urlparse untuk mendapatkan source (nama domain)\n",
        "        args_list = [(url, urlparse(url).netloc) for url in urls_to_scrape]\n",
        "        results = list(executor.map(scrape_wrapper, args_list))\n",
        "        for data in results:\n",
        "            if data:\n",
        "                berita_list.append(data)\n",
        "\n",
        "    df = pd.DataFrame(berita_list, columns=[\n",
        "        \"title\", \"source\", \"date\", \"time\", \"category\", \"sub-category\", \"content\", \"url\"\n",
        "    ])\n",
        "\n",
        "    # Simpan hasil ke CSV\n",
        "    print(f\"➡️ Menyimpan hasil ke {output_csv}...\")\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    print(f\"✅ CSV disimpan! Jumlah artikel berhasil di-scrape: {len(df)}\")\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "93525a52",
      "metadata": {
        "id": "93525a52"
      },
      "outputs": [],
      "source": [
        "# Contoh penggunaan fungsi full_rss_scrape:\n",
        "\n",
        "# Ambil daftar link RSS dari file CSV (misalnya hanya untuk 'tribun')\n",
        "dataset = pd.read_csv('https://raw.githubusercontent.com/gikirima/indonews-scrapper/refs/heads/main/link_scrapping.csv')\n",
        "tribun_rss_links = dataset[dataset['website'] == 'tribun']['link'].tolist()\n",
        "sindonews_rss_links = dataset[dataset['website'] == 'sindonews']['link'].tolist()\n",
        "liputan6_rss_links = dataset[dataset['website'] == 'liputan6']['link'].tolist()\n",
        "detik_rss_links = dataset[dataset['website'] == 'detik']['link'].tolist()\n",
        "kapanlagi_rss_links = dataset[dataset['website'] == 'kapanlagi']['link'].tolist()\n",
        "fimela_rss_links = dataset[dataset['website'] == 'fimela']['link'].tolist()\n",
        "okezone_rss_links = dataset[dataset['website'] == 'okezone']['link'].tolist()\n",
        "posmetro_rss_links = dataset[dataset['website'] == 'posmetro']['link'].tolist()\n",
        "kompas_rss_links = dataset[dataset['website'] == 'kompas']['link'].tolist()\n",
        "republika_rss_links = dataset[dataset['website'] == 'republika']['link'].tolist()\n",
        "tempo_rss_links = dataset[dataset['website'] == 'tempo']['link'].tolist()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Panggil fungsi full_rss_scrape dengan daftar link RSS\n",
        "# Anda bisa menyesuaikan nama file output CSV dan jumlah maksimum artikel\n",
        "df_tribun = full_rss_scrape(tribun_rss_links, output_csv=\"berita_tribun.csv\", max_articles=500)\n",
        "\n",
        "# Jika Anda ingin melihat DataFrame hasilnya:\n",
        "display(df_tribun.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJUF-DaAqDWH",
        "outputId": "08381702-44a3-4bfb-9f41-d694dba6e2a8"
      },
      "id": "XJUF-DaAqDWH",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['https://rss.tempo.co/',\n",
              " 'https://rss.tempo.co/politik',\n",
              " 'https://rss.tempo.co/hukum',\n",
              " 'https://rss.tempo.co/ekonomi',\n",
              " 'https://rss.tempo.co/lingkungan',\n",
              " 'https://rss.tempo.co/wawancara',\n",
              " 'https://rss.tempo.co/investigasi',\n",
              " 'https://rss.tempo.co/cekfakta',\n",
              " 'https://rss.tempo.co/tokoh',\n",
              " 'https://rss.tempo.co/newsletter',\n",
              " 'https://rss.tempo.co/info-tempo']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_sindonews = full_rss_scrape(sindonews_rss_links, output_csv=\"berita_sindonews\", max_articles=500)\n",
        "df_liputan6 = full_rss_scrape(liputan6_rss_links, output_csv=\"berita_liputan6\", max_articles=500)\n",
        "df_detik = full_rss_scrape(detik_rss_links, output_csv=\"berita_detik\", max_articles=500)\n",
        "df_kapanlagi = full_rss_scrape(kapanlagi_rss_links, output_csv=\"berita_kapanlagi\", max_articles=500)\n",
        "df_fimela = full_rss_scrape(fimela_rss_links, output_csv=\"berita_fimela\", max_articles=500)\n",
        "df_okezone = full_rss_scrape(okezone_rss_links, output_csv=\"berita_okezone\", max_articles=500)\n",
        "df_posmetro = full_rss_scrape(posmetro_rss_links, output_csv=\"berita_posmetro\", max_articles=500)\n",
        "df_kompas = full_rss_scrape(kompas_rss_links, output_csv=\"berita_kompas\", max_articles=500)\n",
        "df_republika = full_rss_scrape(republika_rss_links, output_csv=\"berita_republika\", max_articles=500)\n",
        "df_tempo = full_rss_scrape(tempo_rss_links, output_csv=\"berita_tempo\", max_articles=500)"
      ],
      "metadata": {
        "id": "w4MLaEXGrEF9"
      },
      "id": "w4MLaEXGrEF9",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}