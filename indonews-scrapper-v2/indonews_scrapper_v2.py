# -*- coding: utf-8 -*-
"""indonews-scrapper-v2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ngk5IImXdSRc2NvhQuVH_JGubE0M-ihK
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install feedparser requests beautifulsoup4 pandas tqdm

import pandas as pd
import requests
from bs4 import BeautifulSoup
import feedparser
from datetime import datetime
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging
from tqdm import tqdm
import logging

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- Konfigurasi Selector untuk setiap situs berita ---
# Catatan: Selector 'isi' di sini adalah parent container. Skrip akan mencari semua tag <p> di dalamnya.
SELECTORS = {
    'detik.com': {
        'judul': 'h1.detail__title',
        'author': 'div.detail__author',
        'date': 'div.detail__date',
        'isi': 'div.detail__body-text.itp_bodycontent'
    },
    'tribunnews.com': {
        'judul': 'h1#arttitle',
        'author': 'div#penulis a',
        'date': '#article > div:nth-child(4) > div.grey.bdr3.pb10.pt10 > time:nth-child(1)',
        'isi': 'div.side-article.txt-article.multi-fontsize'
    },
    'fimela.com': {
        'judul': 'h1.read-page--header--title.entry-title',
        'author': 'span.read-page--header--author__name.fn',
        'date': 'time.read-page--header--author__modified-time.updated',
        'isi': 'div.article-content-body' # Parent, akan dicari <p> di dalamnya
    },
    'kompas.com': {
        'judul': 'h1.read__title',
        'author': 'div.credit-title-name', # Parent, akan dicari <div> di dalamnya
        'date': 'div.read__time',
        'isi': 'div.read__content > div.clearfix'
    },
    'liputan6.com': {
        'judul': 'h1.read-page--header--title.entry-title',
        'author': 'a.read-page-box__author__name',
        'date': 'span.read-page-box__author__updated',
        'isi': 'div.article-content-body__item-content'
    },
    'tempo.co': {
        'judul': 'h1',
        'author': '#__nuxt > div.stylehead > main > div.flex.flex-col.lg\\:flex-row.justify-between.divide-x.divide-neutral-500.space-x-4 > section.space-y-4 > div:nth-child(2) > div > div.flex.flex-col.justify-center > div:nth-child(1) > a > p',
        'date': '#__nuxt > div.stylehead > main > div.flex.flex-col.lg\\:flex-row.justify-between.divide-x.divide-neutral-500.space-x-4 > section.space-y-4 > div.mt-4 > div > div.container.lg\\:w-full.lg\\:mx-0.lg\\:px-0.space-y-4.mt-4 > p',
        'isi': 'div#content-wrapper' # ID diganti dari ”content-wrapper” ke #content-wrapper
    },
    'republika.co.id': {
        'judul': 'h1',
        'author': '#detail > div.main-content__left > div.max-card__title > div > a',
        'date': 'div.date.date-item__headline',
        'isi': 'div.article-content article' # class diganti dari ”article-content” ke .article-content
    },
    'sindonews.com': {
        'judul': 'h1.detail-title',
        'author': 'a[rel="author"]',
        'date': 'div.detail-date-artikel',
        'isi': 'div#detail-desc'
    },
    'okezone.com': {
        'judul': 'div.title-article > h1',
        'author': 'div.journalist > a:nth-child(2)',
        'date': 'div.journalist > span',
        'isi': 'div.c-detail.read'
    },
    'kapanlagi.com': {
        'judul': 'h1.detail__header_title',
        'author': 'a.headermetabox-author-name',
        'date': 'time.headermetabox-datetime-text',
        'isi': 'div.main-content'
    }
}

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

def get_root_domain(url):
    """
    Mengekstrak domain akar dari URL untuk menangani berbagai subdomain.
    Contoh: 'finance.detik.com' -> 'detik.com'
             'www.republika.co.id' -> 'republika.co.id'
    """
    try:
        parsed_url = urlparse(url)
        netloc = parsed_url.netloc

        parts = netloc.split('.')
        # Menangani domain umum seperti .com, .net, .org
        if len(parts) > 2 and parts[-2] not in ['co', 'ac', 'go', 'or', 'web']:
             return '.'.join(parts[-2:])
        # Menangani domain dengan second-level TLD seperti .co.id, .ac.id
        elif len(parts) > 2:
             return '.'.join(parts[-3:])
        else:
             return netloc
    except Exception:
        return None


def get_article_links_from_rss(file_path):
    """
    Membaca file CSV berisi URL RSS, mem-parsing setiap RSS,
    dan mengembalikan satu list berisi link-link artikel.
    """
    logging.info(f"Mengambil link artikel dari RSS di file: {file_path}")
    article_links = set() # Menggunakan set untuk menghindari duplikat
    try:
        rss_links_df = pd.read_csv(file_path, header=None)
        rss_links = rss_links_df[0].tolist()

        for rss_url in tqdm(rss_links, desc="Membaca RSS Feeds"):
            try:
                feed = feedparser.parse(rss_url)
                for entry in feed.entries:
                    article_links.add(entry.link)
            except Exception as e:
                logging.warning(f"Gagal mem-parsing RSS feed {rss_url}: {e}")
                continue
    except FileNotFoundError:
        logging.error(f"File tidak ditemukan: {file_path}")
        return []
    except Exception as e:
        logging.error(f"Terjadi error saat membaca file CSV: {e}")
        return []

    logging.info(f"Total {len(article_links)} link artikel unik ditemukan.")
    return list(article_links)

def scrape_article(url, max_retries=2):
    root_domain = get_root_domain(url)
    if not root_domain or root_domain not in SELECTORS:
        return None

    for attempt in range(max_retries + 1):
        try:
            response = requests.get(url, headers=HEADERS, timeout=30)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            site_selectors = SELECTORS[root_domain]
            judul = soup.select_one(site_selectors['judul'])
            author_raw = soup.select_one(site_selectors['author'])
            tanggal = soup.select_one(site_selectors['date'])
            author = None
            if root_domain == 'kompas.com' and site_selectors['author'] == 'div.credit-title-name':
                author_elements = soup.select(f"{site_selectors['author']} > div")
                author_text = ' '.join([el.get_text(strip=True) for el in author_elements])
                author = author_text if author_text else (author_raw.get_text(strip=True) if author_raw else None)
            else:
                author = author_raw.get_text(strip=True) if author_raw else None
            content_container = soup.select(site_selectors['isi'])
            isi = ''
            if content_container:
                paragraphs = []
                for container in content_container:
                    paragraphs.extend(container.find_all('p', recursive=True))
                isi = '\n'.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])
            if judul and isi:
                return {
                    'sumber': root_domain,
                    'url': url,
                    'judul': judul.get_text(strip=True),
                    'author': author,
                    'tanggal_publikasi': tanggal.get_text(strip=True) if tanggal else None,
                    'isi_berita': isi,
                }
            else:
                return None
        except requests.RequestException as e:
            logging.warning(f"Request error (percobaan {attempt+1}) saat mengakses {url}: {e}")
            if attempt < max_retries:
                time.sleep(2)  # Tunggu 2 detik sebelum mencoba lagi
            else:
                return None
        except Exception as e:
            logging.warning(f"Error lain saat mengakses {url}: {e}")
            return None

def main():
    """
    Fungsi utama untuk menjalankan seluruh proses scraping.
    """
    rss_file_path = 'link_scrapping.csv'
    all_article_links = get_article_links_from_rss(rss_file_path)

    if not all_article_links:
        logging.info("Tidak ada link untuk di-scrape. Proses dihentikan.")
        return

    scraped_data = []
    # Menggunakan ThreadPoolExecutor untuk scraping paralel
    with ThreadPoolExecutor(max_workers=15) as executor:
        # Membuat future object untuk setiap URL
        future_to_url = {executor.submit(scrape_article, url): url for url in all_article_links}

        # Menampilkan progress bar dengan tqdm

        for future in tqdm(as_completed(future_to_url), total=len(all_article_links), desc="Scraping Artikel"):
            result = future.result()
            if result:
                scraped_data.append(result)

    if not scraped_data:
        logging.info("Tidak ada data yang berhasil di-scrape.")
        return

    # Membuat DataFrame dari data yang berhasil di-scrape
    logging.info(f"Membuat DataFrame dari {len(scraped_data)} artikel...")
    df = pd.DataFrame(scraped_data)

    # Menghapus baris dimana 'isi_berita' dan 'judul' kosong atau None
    df.dropna(subset=['isi_berita', 'judul'], inplace=True)
    df = df[df['isi_berita'] != '']
    df.reset_index(drop=True, inplace=True)

    # Mengekspor DataFrame ke file CSV
    today_date = datetime.now().strftime('%Y-%m-%d')
    output_filename = f"scrapping_news_{today_date}.csv"

    try:
        df.to_csv(output_filename, index=False, encoding='utf-8-sig')
        logging.info(f"Proses scraping selesai. Data berhasil disimpan ke file: {output_filename}")
        print(f"\n✅ Scraping Selesai! {len(df)} berita berhasil disimpan di '{output_filename}'.")
    except Exception as e:
        logging.error(f"Gagal menyimpan DataFrame ke CSV: {e}")

if __name__ == '__main__':
    main()

